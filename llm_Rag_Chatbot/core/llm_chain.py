# core/llm_chain.py
import os
import time
import re
import torch
import unicodedata
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from config.config import MODEL_NAME, ATTN_IMPL, MAX_NEW_TOKENS, MULTI_GPU_SHARDING

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CUDA / PyTorch ê°€ì† ê´€ë ¨ ê¸°ë³¸ ì„¤ì •
# - í° í…ì„œ í• ë‹¹ ì•ˆì •í™”, TF32 í—ˆìš©, matmul ì •ë°€ë„ íŠœë‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.setdefault("PYTORCH_ALLOC_CONF", "expandable_segments:True")
torch.backends.cuda.matmul.allow_tf32 = True
torch.set_float32_matmul_precision("high")

def _load_llm():
    """
    LLMê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , ë‹¨ì¼/ë©€í‹° GPUì— ë§ê²Œ ë°°ì¹˜í•œë‹¤.
    - MULTI_GPU_SHARDING=Trueë©´ device_map="auto" ìƒ¤ë”©ê³¼ max_memoryë¡œ CPU ì˜¤í”„ë¡œë”©ì„ ì°¨ë‹¨
    - Falseë©´ ë‹¨ì¼ GPU(cuda:0)ì— ì „ë¶€ ì˜¬ë¦¼
    ë°˜í™˜: (tokenizer, model, embed_device_str)
    """
    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

    # ì‚¬ìš©í•  dtype ê²°ì • (bf16 ì§€ì›ë˜ë©´ bf16, ì•„ë‹ˆë©´ fp16)
    torch_dtype = (
        torch.bfloat16
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
        else torch.float16
    )

    if MULTI_GPU_SHARDING:
        # âœ… ë©€í‹° GPU ìƒ¤ë”©(PCIe) + CPU ì˜¤í”„ë¡œë”© ì°¨ë‹¨
        max_memory = {}
        n = torch.cuda.device_count()
        for i in range(n):
            max_memory[i] = "78GiB"  # ê° GPUì— í—ˆìš©í•  ìµœëŒ€ ë©”ëª¨ë¦¬(í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆ)

        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",          # í•˜ìœ„ ëª¨ë“ˆì„ ì—¬ëŸ¬ GPUë¡œ ë¶„ì‚°(ìƒ¤ë”©)
            torch_dtype=torch_dtype,    # â— dtype â†’ torch_dtype ë¡œ ë³€ê²½
            low_cpu_mem_usage=True,
            attn_implementation=ATTN_IMPL,  # "sdpa" ë˜ëŠ” "flash_attention_2" ë“±
            max_memory=max_memory,          # CPUë¡œì˜ ì•”ë¬µì  ì˜¤í”„ë¡œë”© ë°©ì§€
            offload_folder=None,            # ë””ìŠ¤í¬ ì˜¤í”„ë¡œë”© ë°©ì§€
        )
        embed_dev = _pick_embed_device(model)  # ì„ë² ë”© ë ˆì´ì–´ì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¥¼ ì°¾ì•„ ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤ë¥¼ ë§ì¶¤
    else:
        # âœ… ë‹¨ì¼ GPU(cuda:0) ê³ ì •
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map=None,          # ë‹¨ì¼ ì¥ì¹˜ì— ìˆ˜ë™ ë°°ì¹˜
            torch_dtype=torch_dtype,  # â— ì—¬ê¸°ì„œë„ torch_dtype ì‚¬ìš©
            low_cpu_mem_usage=True,
            attn_implementation=ATTN_IMPL,
        ).to("cuda:0")
        embed_dev = "cuda:0"

    # ìƒì„± ê´€ë ¨ ì„¸ì´í”„ê°€ë“œ ì„¤ì •
    model.eval()
    model.generation_config.pad_token_id = tok.eos_token_id
    model.generation_config.eos_token_id = tok.eos_token_id
    model.generation_config.use_cache = True

    return tok, model, embed_dev


def _pick_embed_device(model) -> str:
    """
    ìƒ¤ë”©ëœ ëª¨ë¸ì—ì„œ ì„ë² ë”© ë ˆì´ì–´ê°€ ì˜¬ë ¤ì§„ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¥¼ ì¶”ì¶œí•œë‹¤.
    - ì…ë ¥ í…ì„œë¥¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë³´ë‚¸ ë’¤ generate() í˜¸ì¶œ ì‹œ ë””ë°”ì´ìŠ¤ mismatchë¥¼ ë°©ì§€
    """
    dev = "cuda:0"
    try:
        dm = getattr(model, "hf_device_map", None)
        if isinstance(dm, dict):
            v = dm.get("model.embed_tokens", next(iter(dm.values())))
            if isinstance(v, str) and v.startswith("cuda"):
                dev = v
            else:
                # ì˜ˆ: 'cuda:1' ê°™ì€ ë¬¸ìì—´ì„ íŒŒì‹±
                dev = f"cuda:{int(str(v).split(':')[-1])}"
    except Exception:
        pass
    return dev


def _format_docs(docs):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(docs)ì˜ page_contentë¥¼ ì´ì–´ë¶™ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë§Œë“ ë‹¤.
    """
    return "\n\n".join(getattr(d, "page_content", str(d)) for d in (docs or []))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•œêµ­ì–´ í›„ì²˜ë¦¬ ê´€ë ¨ ì •ê·œì‹ ë° í† í° ì…‹
# - ì›í˜• ìˆ«ì/ë¶ˆë¦¿/LaTeX ìˆ˜ì‹/ë‹¤ì¤‘ ê³µë°± ì œê±° ë“±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_CIRCLED_NUM = re.compile(r"[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]")
_BAD_TOKENS = {"ë˜", "ë˜,", "ê³¼", "ê·¸ë¦¬ê³ ", "ê·¸ë¦¬ê³ ,", "ë˜í•œ", "ë˜í•œ,"}
_LATEX = re.compile(r"(\$[^$]+\$|\\\([^\)]+\\\)|\\\[.*?\\\])", re.DOTALL)
_BULLET = re.compile(r"^[\sâ€¢\-Â·*â—‹â—â—¦â– â–¡â˜…â˜†ãƒ»]+")
_MULTI_WS = re.compile(r"\s{2,}")  # â† ë‹¤ì¤‘ ê³µë°± ì¶•ì†Œìš©

def _to_nfkc(s: str) -> str:
    """ìœ ë‹ˆì½”ë“œ NFKC ì •ê·œí™”(ì „ê°/í˜¸í™˜ë¬¸ì â†’ ë³´í†µë¬¸ì ë“±)."""
    return unicodedata.normalize("NFKC", s or "")

def _strip_latex(s: str) -> str:
    """LaTeX/ìˆ˜ì‹ í† í°ì„ '(ìˆ˜ì‹ ìƒëµ)'ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ì›í˜• ë…¸ì¶œì„ ë°©ì§€."""
    return _LATEX.sub("(ìˆ˜ì‹ ìƒëµ)", s)

def _clean_ko_lines(s: str) -> str:
    """
    ì¤„ ë‹¨ìœ„ë¡œ ë¶ˆë¦¿/ì ‘ì†ë¶€ì‚¬ ë‹¨ë… ë¼ì¸ ì œê±° â†’ í•œ ì¤„ë¡œ í•©ì¹¨ â†’ ê³µë°±/ë§ˆì¹¨í‘œ ë³´ê°•.
    """
    lines = []
    for l in s.splitlines():
        l = _BULLET.sub("", l).strip()  # ì•ìª½ ë¶ˆë¦¿/íŠ¹ìˆ˜ë¬¸ì ì œê±°
        if not l:
            continue
        if l in {"ë˜", "ë˜,", "ê·¸ë¦¬ê³ ", "ê·¸ë¦¬ê³ ,", "ë˜í•œ", "ë˜í•œ,"}:  # ì ‘ì†ë¶€ì‚¬ ë‹¨ë… ë¼ì¸ ì œê±°
            continue
        lines.append(l)

    s = " ".join(lines)
    s = _MULTI_WS.sub(" ", s).strip()  # ë‹¤ì¤‘ ê³µë°± ì¶•ì†Œ
    # ë§ˆì§€ë§‰ì— ì™„ê²° ë¶€í˜¸ê°€ ì—†ë‹¤ë©´ ë§ˆì¹¨í‘œ ë³´ê°•(ê²½ìš°ì— ë”°ë¼ ì·¨í–¥ ì¡°ì ˆ)
    if s and not re.search(r"[.?!ë‹¤]$", s):
        s += "."
    return s

def _postprocess_answer(raw: str) -> str:
    """
    ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
    - ìœ ë‹ˆì½”ë“œ ì •ê·œí™” â†’ LaTeX ì¹˜í™˜ â†’ ì¼ë¶€ ìˆ«ì+ëª…ì‚¬ ë¶™ì„ ë³´ì • â†’ í•œêµ­ì–´ ë¼ì¸ ì •ë¦¬
    """
    s = _to_nfkc(raw)
    s = _strip_latex(s)
    s = re.sub(r"(\d+)\s*(ë¬¸ì„œ|íŠ¹ì„±|í˜•ì‹)", r"\1 \2", s)  # ìˆ«ì-ëª…ì‚¬ ë¶™ìŒ ë³´ì • ì˜ˆì‹œ
    s = _clean_ko_lines(s)
    return s


def build_chain(retriever):
    """
    LangChain Runnableì„ ìƒì„±í•œë‹¤.
    - RAG íë¦„: retrieverë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ HF ëª¨ë¸ë¡œ generate â†’ í›„ì²˜ë¦¬ â†’ ë‹µë³€/ì¶œì²˜ ë°˜í™˜
    """
    tok, model, embed_dev = _load_llm()

    # í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ë¬¸ì¥í˜• ê°•ì œ + LaTeX ê¸ˆì§€ + ë‹¨ë½í˜• ìš”ì•½ ì§€ì‹œ + ê·¼ê±°ë¶€ì¬ì‹œ ëŒ€ì‘
    qa_prompt = PromptTemplate(
        template=(
            "[ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ]\n"
            "ì—­í• : ë„ˆëŠ” í•œê¸€ ì„¤ëª… ì „ìš© ë„ìš°ë¯¸ë‹¤.\n"
            "ê·œì¹™:\n"
            "1) ì¶œë ¥ì€ **ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥í˜•**ìœ¼ë¡œë§Œ ì‘ì„±í•œë‹¤. (ì˜ì–´, ë¡œë§ˆì, ì›ë¬¸ ì¸ìš© ê¸ˆì§€)\n"
            "2) ë¬¸ì„œì— ìˆ˜ì‹/LaTeXê°€ ìˆì–´ë„ **ìš°ë¦¬ë§ë¡œ í’€ì–´ì„œ** ì„¤ëª…í•œë‹¤. `$...$`, `\\frac{{}}` ë“± ì›í˜•ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•ŠëŠ”ë‹¤.\n"
            "3) ë¶ˆë¦¿/ëª©ë¡ ëŒ€ì‹  1~2ê°œ **ì™„ê²° ë¬¸ë‹¨**ìœ¼ë¡œ ìš”ì•½í•œë‹¤. (ëŠê¸´ ì–´êµ¬/ì ‘ì†ë¶€ì‚¬ ë‹¨ë… ê¸ˆì§€)\n"
            "4) ê·¼ê±°ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ 'ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•œë‹¤.\n\n"
            "[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]"
        ),
        input_variables=["context", "question"],
    )

    def _qa_runner(inp: dict) -> dict:
        """
        RunnableLambda ì‹¤í–‰ ë³¸ì²´:
        1) retrieverë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        2) í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        3) HF ëª¨ë¸ generate
        4) í•œêµ­ì–´ í›„ì²˜ë¦¬
        5) ë‹µë³€/ì¶œì²˜ ë°˜í™˜
        """
        try:
            q = (inp or {}).get("question", "").strip()
            t0 = time.time()

            # 1) ê²€ìƒ‰
            try:
                docs = retriever.invoke(q)  # LangChain 0.3 Runnable ì¸í„°í˜ì´ìŠ¤
            except Exception:
                docs = retriever._get_relevant_documents(q)  # êµ¬ë²„ì „ í˜¸í™˜ ë°±ì—…
            t1 = time.time()

            # 2) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            ctx = _format_docs(docs)
            prompt_text = qa_prompt.format(context=ctx, question=q)

            # 3) generate (AMPë¡œ ì—°ì‚° ê°€ì†)
            with torch.inference_mode(), torch.cuda.amp.autocast(
                dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)
            ):
                inputs = tok(prompt_text, return_tensors="pt")
                # ìƒ¤ë”© í™˜ê²½ì—ì„œëŠ” ì„ë² ë”© ë ˆì´ì–´ê°€ ì˜¬ë ¤ì§„ ë””ë°”ì´ìŠ¤ë¡œ ì…ë ¥ í…ì„œë¥¼ ì´ë™
                inputs = {k: v.to(embed_dev) for k, v in inputs.items()}
                g0 = time.time()
                out_ids = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,            # ê·¸ë¦¬ë””/ë¹” ê³„ì—´ë¡œ ì¼ê´€ì„± í™•ë³´
                    temperature=0.2,            # ì¼ë¶€ ëª¨ë¸ì—ì„œ ë¡œì§“ ì²˜ë¦¬ì— ì˜í–¥(ì•ˆì •ì„±)
                    top_p=0.9,
                    repetition_penalty=1.1,     # ë°˜ë³µ ì–µì œ
                    eos_token_id=tok.eos_token_id,
                    pad_token_id=tok.eos_token_id,
                )
                g1 = time.time()

            # 4) ë””ì½”ë“œ ë° í”„ë¡¬í”„íŠ¸ ì—ì½” ì œê±°
            text = tok.decode(out_ids[0], skip_special_tokens=True)
            raw_answer = text[len(prompt_text):].strip() if text.startswith(prompt_text) else text.strip()

            # ğŸ”¸ í›„ì²˜ë¦¬(í•œêµ­ì–´ í´ë¦°ì—…)
            # NOTE: í˜„ì¬ íŒŒì¼ì—” _clean_koê°€ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê³ , _postprocess_answerê°€ ì¤€ë¹„ë˜ì–´ ìˆìŒ.
            # ì•„ë˜ í•œ ì¤„ì€ _postprocess_answerë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ì¼ê´€ì„±ì— ë§ìŒ.
            # answer = _clean_ko(raw_answer)
            answer = _postprocess_answer(raw_answer)

            # 5) ì„±ëŠ¥ ë¡œê·¸ + ë°˜í™˜
            total = g1 - t0
            print(f"[perf] retrieve={t1-t0:.2f}s, generate={g1-g0:.2f}s, total={total:.2f}s")
            return {"answer": answer, "source_documents": docs}
        except Exception:
            import traceback
            return {"answer": "[ì²´ì¸ ë‚´ë¶€ ì˜ˆì™¸]\n" + traceback.format_exc(), "source_documents": []}

    # LangChain Runnableë¡œ ì²´ì¸ êµ¬ì„±
    chain = RunnableLambda(_qa_runner)
    print("âœ… LLM ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ìƒ¤ë”©/ì˜¤í”„ë¡œë”© ë°©ì§€ + generate ì§í–‰ + í•œêµ­ì–´ í´ë¦°ì—…)")
    return chain
